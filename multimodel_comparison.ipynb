{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import base64\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, Owlv2ForObjectDetection, Owlv2Processor\n",
    "from transformers.utils.constants import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n",
    "from torchvision.models.detection.faster_rcnn import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the environment variables from dev.env\n",
    "dotenv_path = \".\\\\dev.env\"\n",
    "load_dotenv(dotenv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Define Method to encode Input Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Define Method to feed Images to GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4(input_image):\n",
    "    \n",
    "    base64_image = encode_image(input_image)\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4-turbo\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Do you detect rust on this picture? Or do you see another anomaly?\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 400\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    choices = response.json()['choices']\n",
    "    message = choices[0]['message']\n",
    "    content = message['content']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Define Method to feed Images to GPT4-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GPT4o(input_image):\n",
    "    \n",
    "    base64_image = encode_image(input_image)\n",
    "\n",
    "    client = OpenAI()\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"gpt-4o\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Do you detect rust on this picture? Or do you see another anomaly?\"},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 400\n",
    "    }\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\"}\n",
    "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
    "    choices = response.json()['choices']\n",
    "    message = choices[0]['message']\n",
    "    content = message['content']\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Define Method to feed Images to CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(input_image, sensitivity):\n",
    "# Use GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    input_image = Image.open(input_image)\n",
    "\n",
    "    processor = Owlv2Processor.from_pretrained(\"google/owlv2-large-patch14-finetuned\")\n",
    "    model = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-large-patch14-finetuned\").to(device)\n",
    "\n",
    "    texts = [[\"corrosion or rust\", \"birds nest\"]]\n",
    "    inputs = processor(text=texts, images=input_image, return_tensors=\"pt\").to(device)\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    def get_preprocessed_image(pixel_values):\n",
    "        pixel_values = pixel_values.squeeze().numpy()\n",
    "        unnormalized_image = (pixel_values * np.array(OPENAI_CLIP_STD)[:, None, None]) + np.array(OPENAI_CLIP_MEAN)[:, None, None]\n",
    "        unnormalized_image = (unnormalized_image * 255).astype(np.uint8)\n",
    "        unnormalized_image = np.moveaxis(unnormalized_image, 0, -1)\n",
    "        unnormalized_image = Image.fromarray(unnormalized_image)\n",
    "        return unnormalized_image\n",
    "\n",
    "    unnormalized_image = get_preprocessed_image(inputs.pixel_values.cpu())\n",
    "    target_sizes = torch.Tensor([unnormalized_image.size[::-1]])\n",
    "    # Convert outputs (bounding boxes and class logits) to final bounding boxes and scores\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs=outputs, threshold=sensitivity, target_sizes=target_sizes\n",
    "    )\n",
    "\n",
    "    i = 0  # Retrieve predictions for the first image for the corresponding text queries\n",
    "    text = texts[i]\n",
    "    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n",
    "\n",
    "    draw = ImageDraw.Draw(unnormalized_image)\n",
    "\n",
    "    for box, score, label in zip(boxes, scores, labels):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        st.write(f\"Detected {text[label]} with confidence {round(score.item(), 3)}\")\n",
    "        draw.rectangle(box, outline=\"red\", width=4)\n",
    "        category = 'Cat:' + str(label.item())\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=15)\n",
    "        text_position = (box[0]+10, box[3]-40)\n",
    "        draw.text(text_position, category, fill=\"red\", font=font)\n",
    "    \n",
    "    new_size = (512, 512)\n",
    "    resized_image = unnormalized_image.resize(new_size)\n",
    "\n",
    "    return resized_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Define Method to feed Images to RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet(input_image, sensitivity_resnet):\n",
    "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    num_classes = 5  \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    model.load_state_dict(torch.load('model/model_weights.pth'))\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(input_image).convert('RGB')\n",
    "\n",
    "    # Convert the PIL Image to a PyTorch Tensor\n",
    "    # Instead of manually converting and normalizing, let's use torchvision transforms\n",
    "    transform = T.Compose([ T.ToTensor() ])\n",
    "\n",
    "    img_tensor = transform(img)\n",
    "\n",
    "    # Add a batch dimension since PyTorch models expect batches\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    # Determine the device dynamically\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    img_tensor = img_tensor.to(device)  # Move tensor to the appropriate device\n",
    "\n",
    "    # Ensure the model is on the same device and set it to evaluation mode\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Perform the prediction\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "\n",
    "    # Process the prediction output as needed\n",
    "    im = TF.to_pil_image(img_tensor.squeeze().cpu())\n",
    "    draw = ImageDraw.Draw(im)\n",
    "    st.write(f\"Detected {prediction[0]['scores']}\")\n",
    "\n",
    "    for index, box in enumerate(prediction[0]['boxes'].cpu().numpy()):\n",
    "        if prediction[0]['scores'][index] > sensitivity_resnet:\n",
    "            draw.rectangle(box, width=3, outline=\"red\")\n",
    "            text = str(prediction[0]['labels'][index].item())\n",
    "            text = text + ' score: ' + str(round(prediction[0]['scores'][index].item(),2))\n",
    "            font = ImageFont.truetype(\"arial.ttf\", size=10)\n",
    "            text_position = (box[0], box[3])\n",
    "            draw.text(text_position, text, fill=\"red\", font=font)\n",
    "\n",
    "    return im\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Build and Run the User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(layout=\"wide\")\n",
    "st.title(\"High Voltage Tower Monitor\")\n",
    "\n",
    "with st.sidebar:\n",
    "   st.image('D:/Projects/Tower/elia.png')\n",
    "\n",
    "   st.subheader(\"Upload Image\")\n",
    "   image = st.file_uploader('Chose file')\n",
    "   \n",
    "   st.subheader(\"Choose Model\")\n",
    "   check1 = st.checkbox('ResNet')\n",
    "   slider2 = st.slider('Resnet Sesitivity:', value=80)\n",
    "   check3 = st.checkbox('CLIP')\n",
    "   slider1 = st.slider('CLIP Sesitivity:', value=8)\n",
    "   check2 = st.checkbox('GPT4-Turbo')\n",
    "   check3 = st.checkbox('GPT4o')\n",
    "\n",
    "   button = st.button(\"Submit\")\n",
    "\n",
    "\n",
    "if button:\n",
    "    col1 , col2 = st.columns(2)\n",
    "\n",
    "    with col1:\n",
    "        if image:\n",
    "            st.subheader(\"Original Image\")\n",
    "            st.image(image)\n",
    "            image_path = f\"D:/Projects/Tower3/{image.name}\"\n",
    "    with col2:\n",
    "        if check1:\n",
    "            st.subheader(\"ResNet\")\n",
    "            st.image(resnet(image_path, slider2/100))\n",
    "        if check3:\n",
    "            st.subheader(\"CLIP\") \n",
    "            st.image(clip(image_path, slider1/100))\n",
    "    with col1:\n",
    "        if check2:\n",
    "            st.subheader(\"GPT4-Turbo\")\n",
    "            st.write(GPT4(image_path))\n",
    "        if check3:\n",
    "            st.subheader(\"GPT4o\")\n",
    "            st.write(GPT4o(image_path))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
